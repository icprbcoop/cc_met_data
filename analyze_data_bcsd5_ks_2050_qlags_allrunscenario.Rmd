---
title: "Bureau of Reclamation Projections"
author: "Cherie Schultz"
date: "Sep-Oct, 2019"
output:
  html_document:
    df_print: paged
  word_document: default
subtile: Results from BCSD data
---
```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Initial setup; key inputs
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

library(tidyverse)
library(dplyr)
library(RcppRoll)
library(data.table)
library(zoo)
library(ggplot2)
library(trend)
#
source("code/functions/read_br_spatialave_func.R", local = TRUE)

# By hand filtering criterias ---------------------------------
pcriteria <- 0.10 # preliminary criteria
tcriteria <- 0.10

# K-S test criteria for both p and t
ks_crit <- 0.10
```

```{r setup, include=FALSE}
# This is for setting global options, but don't think it is being used here
knitr::opts_chunk$set(echo = FALSE)
```
## Version notes
Here 
* The annual mean flow climate sensitivity model used is q/q0 = (t-t0) + p/p0 + (p/p0)^2 + qlag
* Simulated annual flow time series are sorted and categorized by long-term mean flow in era - 2050 (an average of 2035-2064 flows); future flow scenarios are currently long-term mean flows centered around (+7.5 and - 7.5 percent)
    + 10th percentile (very dry)
    + 25th percentile (moderately dry)
    + 50th percentile (no change in mean flows)
    + 75th percentile (moderately wet)
    + 90th percentile (very wet)
* The Kolmogorov-Smirnov test is used to filter the runs based on precipitation and temperature time series and a criteria of p-value <= 0.1. But no runs fail with this criteria - need to study!
* R's lm is used to look for flow trends in period, 1950-2017, for each scenario's ensemble of runs. These are compared with an assumed slope of the observed data of zero. Thus the no change scenario is selected. Right now this scenario reduces annual flow in the drought of record by 24%. (I've failed to get a fancier method to discern differences in the observed versus the scenarios - I think the problem is the observed trend has a very large confidence interval. So I need to declare the observed trend to be zero - supported by Hirsch analyses.)

## Import Data

The Bureau of Reclamation (BR) climate change website, https://gdo-dcp.ucllnl.org/downscaled_cmip_projections/dcpInterface.html, provides easy access to a variety of climate change projections. Their baseline period is 1970-1999 and their simulation period is 1950-2099. To begin with, I want to focus on the BCSD (bias-corrected statistically downscaled) and raw GCM data. These are both monthly datasets, so each can be downloaded via a single data request. We also add the observed time series provided by the BR website, which extends from 1950 through 1999, based on Mauer, 2002. I have downloaded time series for spatially averaged results, where the averaged area is the Potomac River watershed above the USGS's Little Falls gage (lat/long input as 38.9375/-77.1875).

### BCSD data
The BCSD data was downloaded on July 31, 2019, by C. Schultz. The provided directories were /1_8obs (Prcp_SpatialStat_mean.csv and Tavg_SpatialStat_mean.csv) for the 1/8th degree observed data and /bcsd5 (pr_SpatialStat_mean.csv and tas_SpatialStat_mean.csv) for the 1/8th degree bias-corrected downscaled CMIP5 data. 
[The file, COLS_SpatialStat.txt, is supposed to list the run names, but I discovered there was a mismatch between the number of columns in the precip and temp data files and the number of names in COLS_SpatialStat.txt. I inquired at BR, and was told on Aug 6 by Tom Pruitt that I'd found a bug. He said that Precip and Tave had more runs than Tmin and Tmax. He said I could find a more complete list of runs in Projections.txt. So I have created  COLS_SpatialStat_pr_tas.txt for use with Precip and Tave.]

### PRISM data
Alimatou Seck downloaded average monthly temperature and precipitation gridded data for the time period, Oct 1895, through Aug 2018 from Oregon State's PRISM website at http://www.prism.oregonstate.edu/. She then computed area-weighted averages for Potomac basin sub-basins, and for the entire watershed above Little Falls. Temperature is in degrees Celsius and precipitation is in millimeters per month. [I need to find the metadata for this download - for grid size, base period.] Because the BR observed datasets only extend up through 1999, and my tentative baseline period is 1975-2004, I am instead using time series derived from the PRISM datasets for comparison's of simulated versus observed temperature and precipitation.

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Read the data files that are dataset specific
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# The following depend on the dataset -----------------------------------------
ts_path <- "input/bcsd_cmip5" # bcsd_cmip5 or gcm_cmip5 or gcm_cmip5
dataset <- "bcsd5" # bcsd5 or bc5 or rgrd5

# Other global inputs ---------------------------------------------------------
ts_output <- "output"

# Read BR precip --------------------------------------------------------------
#    - projections & observed monthly average precipitation rate (mm/day)
#    (the function creates a df with columns: year, month, run, val, type)

precip.df <- read_br_spatialave_func(ts_path,
                                     datatype = "pave",
                                          subfolder_obs = "1_8obs",
                                          subfolder_proj = dataset,
                                          observed_data_id = "Prcp",
                                          projection_data_id = "pr")

# Read temp -------------------------------------------------------------------
#    - projections & observed monthly average surface air temp (deg C)

tave.df <- read_br_spatialave_func(ts_path,
                                     datatype = "tave",
                                          subfolder_obs = "1_8obs",
                                          subfolder_proj = dataset,
                                          observed_data_id = "Tavg",
                                          projection_data_id = "tas")
```

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Read other data files and create master monthly met data df - met.df
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Read days_in_month.csv ------------------------------------------------------ 
#   (created in Excel: number of days in each month from Jan 1950 - Dec 2099)
days_in_month.df <- file.path("data/days_in_month.csv") %>%
    data.table::fread(
      data.table = FALSE,
      header = TRUE,
      showProgress = FALSE)

# Read LFalls-natural reconstructed flows -------------------------------------
#    - put together in ClimateResponseFtns_LFallsNat*.xlsx
#    - right now 1895-2017, monthly, cfs

lfalls_nat.df <- file.path("input/LFalls_nat_recon_monthly.csv") %>%
    data.table::fread(
      data.table = FALSE,
      header = TRUE,
      showProgress = FALSE)
lfalls_nat.df <- as.data.frame(lfalls_nat.df)

# Compute lfalls_annual.df0 ---------------------------------------------------
cfs_to_mmperyear <- 0.02983 # for Little Falls watershed area = 3.223E+11 ft2
                            # (see ClimateResponseFtns_LFallsNat.xlsx)

lfalls_annual.df0 <- lfalls_nat.df %>%
  group_by(year) %>%
  summarise(qave_annual_cfs = mean(lfalls_nat_cfs)) %>%
  dplyr::mutate(q_mm = round(
    cfs_to_mmperyear*qave_annual_cfs, 2)) %>%
  ungroup()

# Read the PRISM data ---------------------------------------------------
# Units of precip are mm per month, so convert to mm/day to match BR data
# For some reason these files have only year >= 1950 - why not earlier?
prism_path <- "input"
p.prism.df0 <- file.path(prism_path, "precip_monthly_mm_prism.csv") %>%
    data.table::fread(
      data.table = FALSE,
      header = TRUE,
      showProgress = FALSE)
p.prism.df <- p.prism.df0 %>%
  dplyr::mutate(pmave = basin_ave) %>%
  dplyr::right_join(days_in_month.df, by = c("year", "month")) %>%
  dplyr::mutate(pmave = pmave/days_in_month) %>%
  dplyr::select(year, month, pmave)

t.prism.df0 <- file.path(prism_path, "temp_monthly_degc_prism.csv") %>%
    data.table::fread(
      data.table = FALSE,
      header = TRUE,
      showProgress = FALSE)
t.prism.df <- t.prism.df0 %>%
  dplyr::mutate(tmave = basin_ave) %>%
  dplyr::select(year, month, tmave)

prism.df <- left_join(p.prism.df, t.prism.df, by = c("year", "month"))
prism.df <- prism.df %>%
  dplyr::mutate(run = "obs_prism", rcp = "obs_prism") %>%
  dplyr::select(year, month, run, rcp, pmave, tmave)

# Create rcp category in BR precip data df ------------------------------------
precip.df <- precip.df %>%
  mutate(pave = val,
         rcp = case_when(
           str_detect(run, "rcp26") == TRUE ~ "rcp26",
           str_detect(run, "rcp45") == TRUE ~ "rcp45",
           str_detect(run, "rcp60") == TRUE ~ "rcp60",
           str_detect(run, "rcp85") == TRUE ~ "rcp85",
           str_detect(run, "obs") == TRUE ~ "obs_br",
  TRUE ~ "NA_what")
         ) %>%
  select(year, month, run, rcp, pave)

tave.df <- tave.df %>%
  mutate(tave = val) %>%
  select(year, month, run, tave)

# Combine BR precip & temp data ----------------------------------------------- 
#    (met.df cols: year, month, run, rcp, 
#                  pmave (monthly, mm/day), tmave (monthly, deg C) )
met.df <- left_join(precip.df, tave.df, by = c("year", "month", "run"))
met.df <- met.df %>%
  dplyr::mutate(pmave = pave, tmave = tave) %>%
  dplyr::select(-pave, -tave) %>%
  dplyr::filter(pmave != "NA")

# Add the PRISM data to the BR data; discard dates with no data ---------------
met.df <- bind_rows(met.df, prism.df) %>%
  dplyr::filter(year != "NA") %>%
  dplyr::filter(!(run == "obs_prism" & pmave == "NA"))

# Add a column for 30-year eras -----------------------------------------------
# First need to delete years that we aren't using -----------------------------
# Also delete era = 2020 for obs_prism - since only 12 years
met.df <- met.df %>%
  dplyr::filter(year >= 1950 & year < 2095) %>%
  dplyr::mutate(era = as.integer(case_when(
    year >= 1950 & year < 1980 ~ 1965, # base period, if delta q?
    # year >= 1980 & year < 2005 ~ 1992, # just 25 years - for verification?
    # year >= 2005 & year < 2035 ~ 2020, 
    # year >= 1980 & year < 2009 ~ 1998, # full recent record
    year >= 1980 & year < 2010 ~ 1995, # next 30 years
    year >= 2010 & year < 2035 ~ 2022, 
    year >= 2035 & year < 2065 ~ 2050, # forecast period
    year >= 2065 & year < 2095 ~ 2080, 
    TRUE ~ -99999
  ))) %>%
  # dplyr::filter(!(rcp == "obs_prism" & era >= 2020)) %>%
  dplyr::select(year, month, era, run, rcp, pmave, tmave)
```

## Characterize meteorological data

Because we are interested in "long-term" average conditions, the record is divided up into "eras", each about 30 years in length. The eras are
- 1950-1979, 30-year "base" era
- 1980-2009, 30-year recent past era
- 2010-2034, 25-year current era
- 2035-2064, 30-year current planning forecast era
- 2065-2094, 30-year long-term planning forecast era

Mean flow at Little Falls for the base era is 350 mm (11,741 cfs), just 0.5% less than mean flow for the period, 1897-1979, 351 mm (11,772 cfs). Mean precipitation is 992 mm in both periods! [The only thing that doesn't match so well is mean temperature: 11.2 deg C for 1896-1979 and 11.0 deg C for 1950-1979.] This latter period, 1896-1979, was used to generate the coefficients for the regression model which predicts flow, as a fraction of mean flow, precipitation as a fraction of mean precipitation, precipitation squared as fraction of mean precipitation squared (ie P^2/P0^2), and change in temperature in degrees Celsius, from mean temperature in 1897-1979. 
```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Examine unfiltered precip & temp data
#   - creating met.annual.df & met.run.ltstats.df
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Create df with annual averages by run by year -------------------------------
#   (note pmave is mm per day)
#   (pave_annual in mm per year and tave_annual in deg C)
met.annual.df0 <- met.df %>%
  dplyr::group_by(year, era, run, rcp) %>%
  dplyr::right_join(days_in_month.df, by = c("year", "month")) %>%
  dplyr::filter(!era=="NA") %>%
  dplyr::mutate(pmave = pmave*days_in_month) %>% # units from mm/day -> mm/month
  dplyr::summarise(pave_annual = sum(pmave),
                   tave_annual = mean(tmave)) %>%
  ungroup()


# Calculate long-term (lt) run stats by era -----------------------------------
  met.run.ltstats.df <- met.annual.df0 %>%
    dplyr::mutate(p = pave_annual, t = tave_annual) %>%
    dplyr::group_by(run, rcp, era) %>%
    dplyr::summarise(pltmean = mean(p),
                     pltsd = sd(p),
                     pltmin = min(p),
                     plt05 = quantile(p, probs = 0.05, na.rm = TRUE),
                     plt10 = quantile(p, probs = 0.1, na.rm = TRUE),
                     plt25 = quantile(p, probs = 0.25, na.rm = TRUE),
                     plt50 = quantile(p, probs = 0.5, na.rm = TRUE),
                     plt75 = quantile(p, probs = 0.75, na.rm = TRUE),
                     plt90 = quantile(p, probs = 0.9, na.rm = TRUE),
                     plt95 = quantile(p, probs = 0.95, na.rm = TRUE),
                     pltmax = max(p),
                     tltmean = mean(t),
                     tltsd = sd(t),
                     tltmin = min(t),
                     tlt05 = quantile(t, probs = 0.05, na.rm = TRUE),
                     tlt10 = quantile(t, probs = 0.1, na.rm = TRUE),
                     tlt25 = quantile(t, probs = 0.25, na.rm = TRUE),
                     tlt50 = quantile(t, probs = 0.5, na.rm = TRUE),
                     tlt75 = quantile(t, probs = 0.75, na.rm = TRUE),
                     tlt90 = quantile(t, probs = 0.9, na.rm = TRUE),
                     tlt95 = quantile(t, probs = 0.95, na.rm = TRUE),
                     tltmax = max(t)) %>%
  dplyr::mutate_at(4:14, round, 0) %>%
  dplyr::mutate_at(15:25, round, 1) %>%
    dplyr::arrange(rcp, run, era) %>% # so rcp = "obs", era = 1990 is in row 3
  dplyr::ungroup()

# Count runs in rcp's ---------------------------------------------------------
rcp.count.df <- met.run.ltstats.df %>%
  dplyr::group_by(rcp) %>%
  dplyr::count(run) %>%
  dplyr::ungroup()
rcp.count.df <- rcp.count.df %>%
  dplyr::group_by(rcp) %>%
  dplyr::count(rcp) %>%
  dplyr::ungroup()

# Compute stats for each rcp --------------------------------------------------
met.rcp.stats.df <- met.run.ltstats.df %>%
  dplyr::group_by(rcp, era) %>%
  dplyr::summarise(pltmin_median = median(pltmin),
                   plt05_median = median(plt05),
                   plt10_median = median(plt10),
                   plt25_median = median(plt25),
                   plt50_median = median(plt50),
                   plt75_median = median(plt75),
                   plt90_median = median(plt90),
                   plt95_median = median(plt95),
                   pltmean_median = median(pltmean),
                   pltmean90 = quantile(pltmean, probs = 0.90, na.rm = TRUE),
                   pltmean75 = quantile(pltmean, probs = 0.75, na.rm = TRUE),
                   pltmean50 = quantile(pltmean, probs = 0.50, na.rm = TRUE),
                   pltmean25 = quantile(pltmean, probs = 0.25, na.rm = TRUE),
                   pltmean10 = quantile(pltmean, probs = 0.10, na.rm = TRUE),
                   pltsd_median = median(pltsd),
                   tlt05_median = median(tlt05),
                   tlt10_median = median(tlt10),
                   tlt25_median = median(tlt25),
                   tlt50_median = median(tlt50),
                   tlt75_median = median(tlt75),
                   tlt90_median = median(tlt90),
                   tlt95_median = median(tlt95),
                   tltmean_median = median(tltmean),
                   tltmean90 = quantile(tltmean, probs = 0.90, na.rm = TRUE),
                   tltmean75 = quantile(tltmean, probs = 0.75, na.rm = TRUE),
                   tltmean50 = quantile(tltmean, probs = 0.50, na.rm = TRUE),
                   tltmean25 = quantile(tltmean, probs = 0.25, na.rm = TRUE),
                   tltmean10 = quantile(tltmean, probs = 0.10, na.rm = TRUE),
                   tltsd_median = median(tltsd),
                   metlt_n = n()
                   ) %>%
  dplyr::mutate_at(3:22, round, 2) %>%
  dplyr::ungroup()

# Save the wide format table for display --------------------------------------
met.rcp.stats.wide <- met.rcp.stats.df  %>%
  dplyr::select(rcp, era, metlt_n, 
                plt05_median, plt25_median, plt50_median, plt75_median, 
                plt95_median, pltmean_median, pltsd_median,
                tlt05_median, tlt25_median, tlt50_median, tlt75_median, 
                tlt95_median, tltmean_median, tltsd_median)

# Switch to long format for graphing ------------------------------------------
met.rcp.stats.df <- met.rcp.stats.df %>%
  tidyr::gather(key = "stat", value = "val", -rcp, -era)

# Take a peek at unfiltered annual data ---------------------------------------
p_mean_sd <- met.rcp.stats.df %>%
  dplyr::filter(stat == "pltsd_median" | stat == "pltmean_median")
p_p10_p90 <- met.rcp.stats.df %>%
  dplyr::filter(stat == "pltmin_median" | stat == "plt10_median" | 
                stat == "plt25_median" | stat == "plt50_median" |
                  stat == "plt75_median" | stat == "plt90_median")

p_mean_10_90 <- met.rcp.stats.df %>%
  dplyr::filter(stat == "pltmean10" | stat == "pltmean25"
                 | stat == "pltmean50" | stat == "pltmean75"
                 | stat == "pltmean90")

t_mean_sd <- met.rcp.stats.df %>%
  dplyr::filter(stat == "tltsd_median" | stat == "tltmean_median")
t_t10_t90 <- met.rcp.stats.df %>%
  dplyr::filter(stat == "tlt10_median" | stat == "tlt25_median" | 
                  stat == "tlt50_median" |
                  stat == "tlt75_median" | stat == "tlt90_median")

t_mean_10_90 <- met.rcp.stats.df %>%
  dplyr::filter(stat == "tltmean10" | stat == "tltmean25"
                 | stat == "tltmean50" | stat == "tltmean75"
                 | stat == "tltmean90")

# and also at monthly data ----------------------------------------------------
met.monthly.df <- met.df %>%
  group_by(month, era, rcp) %>%
  dplyr::right_join(days_in_month.df, by = c("year", "month")) %>%
  dplyr::filter(!era=="NA") %>%
  dplyr::mutate(pmave = pmave*days_in_month) %>% # units from mm/day -> mm/month
  dplyr::summarise(pave_monthly = mean(pmave),
                   tave_monthly = mean(tmave)) %>%
  ungroup()
# Create some monthly df's for graphing ---------------------------------------
p.monthly.rcp <- met.monthly.df %>%
  group_by(month, rcp) %>%
  summarise(pave_monthly = mean(pave_monthly)) %>%
  select(rcp, month, pave_monthly) %>%
  ungroup()
t.monthly.rcp <- met.monthly.df %>%
  group_by(month, rcp) %>%
  summarise(tave_monthly = mean(tave_monthly)) %>%
  select(rcp, month, tave_monthly) %>%
  ungroup()
p.monthly.era <- met.monthly.df %>%
  group_by(month, era) %>%
  summarise(pave_monthly = mean(pave_monthly)) %>%
  select(era, month, pave_monthly) %>%
  ungroup()
t.monthly.era <- met.monthly.df %>%
  group_by(month, era) %>%
  summarise(tave_monthly = mean(tave_monthly)) %>%
  select(era, month, tave_monthly) %>%
  ungroup()
 
```
## Examine stats by rcp of unfiltered run

Below is a table with the number of unfiltered runs for each RCP. Also shown are graphs of predicted trends by RCP. For each run, stats were computed for the 25-30 year "eras". Then the median of the run stats were computed for each RCP and for the observed data.
```{r}
knitr::kable(rcp.count.df)
```

```{r}

# ggplot(data = p_mean_10_90, aes(x = era, y = val))  +
#   geom_line(aes(linetype = rcp, colour = stat)) +
#   ggtitle("Precip quantiles of run lt means by RCP") +
#   scale_y_continuous(name = "Precipitation, mm/year", 
#                      limits = c(900, 1200), breaks = seq(900, 1200, 100)) +
#   scale_x_continuous(breaks = c(1960, 1990, 2020, 2050, 2080))
ggplot(data = p_p10_p90, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Precip medians of run long-term percentiles and minimums by RCP") +
  # labs(x = "Year", y = "Precipitation, mm per year") +
  scale_y_continuous(name = "Precipitation, mm/year", 
                     limits = c(600, 1400), breaks = seq(600, 1400, 200)) +
  scale_x_continuous(name = "Year",
                     breaks = c(1960, 1990, 2020, 2050, 2080))
# ggplot(data = t_mean_10_90, aes(x = era, y = val))  +
#   geom_line(aes(linetype = rcp, colour = stat)) +
#   ggtitle("Temp quantiles of run lt means by RCP") +
#   labs(x = "Year", y = "Temperature, deg C") +
# #    scale_y_continuous(breaks = seq(0, 50, 10)) +
#   scale_x_continuous(breaks = c(1960, 1990, 2020, 2050, 2080))
ggplot(data = t_t10_t90, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Temp medians of run long-term percentiles by RCP") +
  labs(x = "Year", y = "Temperature, deg C")

```

```{r}
# Graph monthly ave data
ggplot(data = p.monthly.rcp, aes(x = month, y = pave_monthly))  +
  geom_line(aes(linetype = rcp)) +
  ggtitle("Precip monthly means by RCP") +
  scale_y_continuous(name = "Precipitation, mm/month", 
                     limits = c(60, 120), breaks = seq(60, 120, 20)) +
  scale_x_continuous(name = "Month", 
                     limits = c(1, 12), breaks = seq(1, 12, 1))
ggplot(data = t.monthly.rcp, aes(x = month, y = tave_monthly))  +
  geom_line(aes(linetype = rcp)) +
  ggtitle("Temp monthly means by RCP") +
  labs(x = "Month", y = "Temperature, deg C")
ggplot(data = p.monthly.era, aes(x = month, y = pave_monthly))  +
  geom_line(aes(colour = factor(era))) +
  ggtitle("Precip monthly means by era") +
  scale_y_continuous(name = "Precipitation, mm/month", 
                     limits = c(60, 120), breaks = seq(60, 120, 20)) +
  scale_x_continuous(name = "Month", 
                     limits = c(1, 12), breaks = seq(1, 12, 1))
ggplot(data = t.monthly.era, aes(x = month, y = tave_monthly))  +
  geom_line(aes(colour = factor(era))) +
  ggtitle("Temp monthly means by era") +
  labs(x = "Month", y = "Temperature, deg C")
```


## Filter runs

Runs are first filtered based on model performance in an historical period, as compared to observed precipitation and temperature, using the Kolmogorov-Smirnow test (stats::ks.test). The NULL hypothesis is that the actual and simulated data come from the same distribution, and the ALTERNATIVE hypothesis is that they are different. So HIGH p-values indicate a decent fit of simulated and actual values.

Using 1950-2017 PRISM observations
- ks_crit = 0.1 eliminates 36 runs
- ks_crit = 0.2 eliminates 102 runs

Using 1950-1999 BR observations
- ks_crit = 0.1 eliminates 0 runs
- ks_crit = 0.2 eliminates 8 runs
- ks_crit = 0.5 eliminates 47 runs
- ks_crit = 0.75 eliminates 167 runs

It seems the BCSD method was done in a way that matched the observed cdf.

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Filter the runs - using Kolmogorov-Smirnov for cdf
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Spreading creates 234 columns: year, [the 231 cc runs], obs, obs_prism
# Can compare with PRISM data (1950-2017) or BR data (1950-1999)

# ks_crit <- 0.10
p_filter.df <- met.annual.df0 %>%
  filter(year < 2000) %>%
  select(year, run, pave_annual) %>%
  spread(key = run, value = pave_annual)

t_filter.df <- met.annual.df0 %>%
  filter(year < 2000) %>%
  select(year, run, tave_annual) %>%
  spread(key = run, value = tave_annual)

# p_obs <- as.vector(p_filter.df$obs_prism)
# t_obs <- as.vector(t_filter.df$obs_prism)
p_obs <- as.vector(p_filter.df$obs)
t_obs <- as.vector(t_filter.df$obs)

# Want a df to hold filter results --------------------------------------------
#   - can't remember elegant way! but this works
filter_stats.df <- data.frame(run = names(p_filter.df[, 2:234]), 
                              ks_test_pval_p = 1.0, 
                              ks_test_D_p = 999.9,
                              ks_test_pval_t = 1.0,
                              ks_test_D_t = 999.9)

# Record Kolmogorov-Smirnov test results - 1 is perfect fit -------------------
for (i in 2:234) {  # last 2 columns are obs (br) and obs_prism
  ks_test_p <- p_filter.df[, i]
  ks_test_p <- unlist(ks_test_p)  
test_p <- ks.test(ks_test_p, p_obs, alternative = "two.sided") 
  ks_test_t <- t_filter.df[, i]
  ks_test_t <- unlist(ks_test_t)  
test_t <- ks.test(ks_test_t, t_obs, alternative = "two.sided") 
filter_stats.df$ks_test_pval_p[i-1] <- test_p$p.value[1]
filter_stats.df$ks_test_D_p[i-1] <- test_p$statistic[1]
filter_stats.df$ks_test_pval_t[i-1] <- test_t$p.value[1]
filter_stats.df$ks_test_D_t[i-1] <- test_t$statistic[1]
}

# If both p and t ks tests good
filter_stats.df <- filter_stats.df %>%
  dplyr::mutate(ks_test = if_else(ks_test_pval_p > ks_crit & ks_test_pval_t > ks_crit, 
                                  "pass", "fail"))
filter_fails <- filter_stats.df %>%
  filter(ks_test == "fail")


```

## Old method of filtering - save
via 10 statistics each for annual average precipitation and annual average temperature:
mean, standard deviation, minimum, maximum, and the following percentiles: 5th, 10th, 25th, 50th, 75th, 90th, 95th.
```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Filter the runs - by hand method
#    but use KS results for final selection
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# For now, use prism data for filtering, 
#    - with era = 1965 (1950-1979) for filtering comparison period
#    - I guess we could define another time period for filtering purposes
#    - maybe go back to 1970-1999?, which I think was used in BCSD downscaling

# pf and tf are obs_prism lt base stats, for comparison with model base stats
# row 3 is era = 1965 obs_prism
pf <- met.run.ltstats.df[3,4:14]
tf <- met.run.ltstats.df[3, 15:25]
#
# Create columns, ptest and ttest = pass, fail --------------------------------
met.run.ltstats.df <- met.run.ltstats.df %>%
    mutate(ptest = "pass",
           ptest = case_when(
             abs(pltmin - pf$pltmin[1])/pf$pltmin[1] > pcriteria ~ "fail",
             abs(plt05 - pf$plt05[1])/pf$plt05[1] > pcriteria ~ "fail",   
             abs(plt10 - pf$plt10[1])/pf$plt10[1] > pcriteria ~ "fail",
             abs(plt25 - pf$plt25[1])/pf$plt25[1] > pcriteria ~ "fail",
             abs(plt50 - pf$plt50[1])/pf$plt50[1] > pcriteria ~ "fail",
             abs(plt75 - pf$plt75[1])/pf$plt75[1] > pcriteria ~ "fail",
             abs(plt90 - pf$plt90[1])/pf$plt90[1] > pcriteria ~ "fail", 
             abs(plt95 - pf$plt95[1])/pf$plt95[1] > pcriteria ~ "fail",
             # abs(pltmax - pf$pltmax[1])/pf$pltmax[1] > pcriteria*2 ~ "fail",
             # abs(pltsd - pf$pltsd[1])/pf$pltsd[1] > pcriteria ~ "fail",
             is.na(pf$pltmean[1]) ~ "fail",
             TRUE ~ ptest),
           ttest = "pass",
           ttest = case_when(
             abs(tltmin - tf$tltmin[1])/tf$tltmin[1] > tcriteria ~ "fail",
             abs(tlt05 - tf$tlt05[1])/tf$tlt05[1] > tcriteria ~ "fail",   
             abs(tlt10 - tf$tlt10[1])/tf$tlt10[1] > tcriteria ~ "fail",
             abs(tlt25 - tf$tlt25[1])/tf$tlt25[1] > tcriteria ~ "fail",
             abs(tlt50 - tf$tlt50[1])/tf$tlt50[1] > tcriteria ~ "fail",
             abs(tlt75 - tf$tlt75[1])/tf$tlt75[1] > tcriteria ~ "fail",
             abs(tlt90 - tf$tlt90[1])/tf$tlt90[1] > tcriteria ~ "fail", 
             abs(tlt95 - tf$tlt95[1])/tf$tlt95[1] > tcriteria ~ "fail",
             abs(tltmax - tf$tltmax[1])/tf$tltmax[1] > tcriteria ~ "fail",
             # abs(tltsd - tf$tltsd[1])/tf$tltsd[1] > tcriteria ~ "fail",
             is.na(tf$tltmean[1]) ~ "fail",
             TRUE ~ ttest),
             filtertest = if_else(ptest == "pass" & ttest == "pass", 
                                  "pass", "fail")
             )
  
# Create df with filtertest for base period for each run ----------------------
#   (for now the base period is 1980-1999, ie era = 1990)
  
annualfilter.base0 <- met.run.ltstats.df %>% # this can be used to select the runs
    dplyr::filter(era == 1965) %>%
    mutate(pltmean_base = pltmean, tltmean_base = tltmean) %>%
    dplyr::select(run, filtertest, pltmean_base, tltmean_base) 

# Add results of ks test
annualfilter.base <- left_join(annualfilter.base0, filter_stats.df, by = "run")
  
# Add the filtertest for the base period to met.run.stats.df,
#   - get rid of failing runs, and pare down number of columns
met.frun.ltstats.small.df <- left_join(met.run.ltstats.df, 
                                      annualfilter.base, by = "run") %>%
    mutate(filtertest = filtertest.y) %>%
    dplyr::filter(ks_test == "pass") %>%
    dplyr::select(run, rcp, era, filtertest, ks_test,
           pltmean_base, pltmean, pltsd, pltmin, plt05, 
           plt25, plt50, plt75, plt95, pltmax,
           tltmean_base, tltmean, tltsd, tltmin, tlt05, 
           tlt25, tlt50, tlt75, tlt95, tltmax)
  
# Compute stats for each filtered rcp -----------------------------------------
met.frcp.stats.df <- met.frun.ltstats.small.df %>%
    dplyr::group_by(rcp, era) %>%
    dplyr::summarise(pltmin_median = median(pltmin),
                     plt05_median = median(plt05),
                     plt25_median = median(plt25),
                     plt50_median = median(plt50),
                     plt75_median = median(plt75),
                     plt95_median = median(plt95),
                     pltmean_median = median(pltmean),
                     pltsd_median = median(pltsd),
                     pltmean90 = quantile(pltmean, probs = 0.90, na.rm = TRUE),
                     pltmean75 = quantile(pltmean, probs = 0.75, na.rm = TRUE),
                     pltmean50 = quantile(pltmean, probs = 0.50, na.rm = TRUE),
                     pltmean25 = quantile(pltmean, probs = 0.25, na.rm = TRUE),
                     pltmean10 = quantile(pltmean, probs = 0.10, na.rm = TRUE),
                     tlt05_median = median(tlt05),
                     tlt25_median = median(tlt25),
                     tlt50_median = median(tlt50),
                     tlt75_median = median(tlt75),
                     tlt95_median = median(tlt95),
                     tltmean_median = median(tltmean),
                     tltsd_median = median(tltsd),
                     tltmean90 = quantile(tltmean, probs = 0.90, na.rm = TRUE),
                     tltmean75 = quantile(tltmean, probs = 0.75, na.rm = TRUE),
                     tltmean50 = quantile(tltmean, probs = 0.50, na.rm = TRUE),
                     tltmean25 = quantile(tltmean, probs = 0.25, na.rm = TRUE),
                     tltmean10 = quantile(tltmean, probs = 0.10, na.rm = TRUE)) %>%
  dplyr::ungroup() %>%
  tidyr::gather(key = "stat", value = "val", -rcp, -era)
  
# Count passing runs by rcp ---------------------------------------------------
rcp.fcount.df <- met.frun.ltstats.small.df %>%
    dplyr::group_by(rcp, ks_test) %>%
    dplyr::filter(ks_test == "pass") %>%
    dplyr::count(run) %>%
    dplyr::ungroup()
rcp.fcount.df <- rcp.fcount.df %>%
    dplyr::group_by(rcp) %>%
    dplyr::count(rcp) %>%
    dplyr::ungroup()
  
# Prepare to graph rcp stats of filtered runs ---------------------------------
p_mean_sd <- met.frcp.stats.df %>%
    dplyr::filter(stat == "pltsd_median" | stat == "pltmean_median")
p_mean_10_90 <- met.frcp.stats.df %>%
  dplyr::filter(stat == "pltmean10" | stat == "pltmean25"
                 | stat == "pltmean50" | stat == "pltmean75"
                 | stat == "pltmean90")
p_p05_p95 <- met.frcp.stats.df %>%
    dplyr::filter(stat == "pltmin_median" | stat == "plt05_median" | 
                    stat == "plt25_median" | stat == "plt50_median" |
                    stat == "plt75_median" | stat == "plt95_median" )
t_mean_sd <- met.frcp.stats.df %>%
    dplyr::filter(stat == "tltsd_median" | stat == "tltmean_median")
t_mean_10_90 <- met.frcp.stats.df %>%
  dplyr::filter(stat == "tltmean10" | stat == "tltmean25"
                 | stat == "tltmean50" | stat == "tpltmean75"
                 | stat == "tltmean90")

t_t05_t95 <- met.frcp.stats.df %>%
    dplyr::filter(stat == "tltmin_median" | stat == "tlt05_median" | 
                    stat == "tlt25_median" | stat == "tlt50_median" |
                    stat == "tlt75_median" | stat == "tlt95_median" )
```
### Results

The filtering criteria is (can change at the beginning of this file): 
  % difference between run stats and observed stats is  < `r pcriteria` for precip and `r tcriteria` for temp. The pmax criteria has been dropped. The total number of passing runs is `r sum(rcp.fcount.df$n[3:6])`. 

But right now the code is ignoring the by-hand filtertest results and just filtering with ks_test.  
  
We look at some graphs of trends as well:
```{r}
knitr::kable(rcp.fcount.df)
```

```{r}
# ggplot(data = p_mean_sd, aes(x = era, y = val))  +
#   geom_line(aes(linetype = rcp, colour = stat)) +
#   ggtitle("Precip medians of filtered run lt means and stdevs by RCP") +
#   labs(x = "Year", y = "Precipitation, mm per year")
ggplot(data = p_mean_10_90, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Precip quantiles of run lt means by RCP") +
  scale_y_continuous(name = "Precipitation, mm/year", 
                     limits = c(900, 1200), breaks = seq(900, 1200, 100)) +
  scale_x_continuous(name = "Year",
                     breaks = c(1960, 1990, 2020, 2050, 2080))
ggplot(data = p_p05_p95, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Precip medians of filtered run lt quantiles and minimums by RCP") +
  scale_y_continuous(name = "Precipitation, mm/year", 
                     limits = c(600, 1400), breaks = seq(600, 1400, 200)) +
  scale_x_continuous(name = "Year",
                     breaks = c(1960, 1990, 2020, 2050, 2080))
ggplot(data = t_mean_10_90, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Temp quantiles of run lt means by RCP") +
  labs(x = "Year", y = "Temperature, deg C")
ggplot(data = t_t05_t95, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Temp medians of filtered run lt quantiles by RCP") +
  labs(x = "Year", y = "Temperature, deg C")
```

## Predict changes in flow

Our simple annual flow sensitivity equation is used to compute average annual flow as a function of average annual temperature and precipitation. No q lag term is present in the regression equation used to generate these preliminary results.

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Add predicted flows
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Add the filtertest info to met annual averages ------------------------------
met.annual.df <-  left_join(met.annual.df0, annualfilter.base, by = "run") %>%
  dplyr::arrange(run, year)
met.fannual.df0 <- met.annual.df %>%
  dplyr::filter(ks_test == "pass")

#******************************************************************************
# Old climate response ftn code - model T3
#******************************************************************************
# # Define the climate response function parameters  ----------------------------
# #  (based for the time being on annual results "T3",
# #   with 1897-79 averages, no q_lag)
# # For some reason I don't remember, this is in terms of qfrac and pfrac
# #    - instead of dqfrac and dpfrac
# 
# # The T3 regression equation is q_frac = b_dt*dt + b_p*p_frac
# # where q, p, and t are annual averages of flow, precip, and temperature, and
# #    qfrac = q/qbar
# #    dt = t - tbar
# #    dpfraq = p/pbar
# qbar <- 350 # mm
# tbar <- 11.19 # deg C
# pbar <- 992 # mm
# b_dp <- 0.12 # not significant in T3 equation but keep it for now
# b_dp2 <- 0.87
# b_dt <- -0.073
# # b_dt <- b_dt/2 # should do some sensitivity tests
# b_0 <- -0.00
# 
# # Compute dq predictions annually ---------------------------------------------
# #   (for each model run for each year)
# q_met.fannual.df00 <- met.fannual.df %>%
#   mutate(pfrac = round(pave_annual/pbar, 3),
#          dt = round((tave_annual - tbar), 3),
#          qfrac = round(pfrac*b_dp + pfrac*pfrac*b_dp2 + dt*b_dt + b_0, 3),
#          pave_annual = round(pave_annual, 0),
#          pltmean_base = round(pltmean_base, 0),
#          tave_annual = round(tave_annual, 2),
#          tltmean_base = round(tltmean_base, 2))
#******************************************************************************
# New climate response ftn code
#******************************************************************************
# Define the climate response function parameters  ----------------------------
#  (based on annual results "T0", developed from 1897-79 mean q frac's)
# T3 eq.: qfrac = b_dt*dt + b_p*pfrac + b_dp2*pfrac^2 + b_qlag*qfrac_lag
# where q, p, and t are annual averages of flow, precip, and temperature, and
# and qbar, tbar, pbar are long-term means for 1897-1979
#    qfrac = q/qbar
#    dt = t - tbar
#    pfraq = p/pbar
#
# Model T0 input parameters ---------------------------------------------------
# qbar <- 350 # mm
# tbar <- 11.19 # deg C
# pbar <- 992 # mm
# b_p <- -0.33
# b_p2 <- 1.084
# b_dt <- -0.080
# b_qlag <- 0.23

# Model T5 input parameters ---------------------------------------------------

qbar <- 350 # mm
tbar <- 11.19 # deg C
pbar <- 992 # mm
b_p <- -0.22
b_p2 <- 1.058
b_dt <- -0.054
b_qlag <- 0.142
# b_0 <- -0.00

set.seed(406)
nerr <- length(met.fannual.df0$run)
err <-  rnorm(nerr, 0, 0.13)
err <- as.data.frame(err)
colnames(err) <- c("err_term")
met.fannual.df <- bind_cols(met.fannual.df0, err)

# Compute qfrac predictions annually ------------------------------------------
#   (for each model run for each year)
q_met.fannual.df00 <- met.fannual.df %>%
  select(-ks_test_pval_p, -ks_test_D_p, -ks_test_pval_t, -ks_test_D_t,
         -filtertest, - ks_test) %>%
  arrange(run, year) %>%
  mutate(pfrac = round(pave_annual/pbar, 3),
         dt = round((tave_annual - tbar), 3),
         qfrac = if_else(year == 1950, 1.00, -9.99),
         qfrac_lag = lag(qfrac, 1),
         noise = rnorm(1, 0, 10))
for (iyear in 1951:2099) {
  q_met.fannual.df00 <- q_met.fannual.df00 %>%
    mutate(qfrac = if_else(year == iyear,
                           b_p*pfrac + b_p2*pfrac^2 + 
                             b_dt*dt + b_qlag*qfrac_lag + err_term,
                           qfrac),
           qfrac_lag = lag(qfrac, 1))
}

#******************************************************************************
# End of new climate response ftn code
#******************************************************************************

# For each run, compute long-term dq stats over each era ----------------------
q.frun.ltstats0.df <- q_met.fannual.df00 %>%
  group_by(run, era) %>%
  summarise(pltmean = round(mean(pave_annual), 0),
            tltmean = round(mean(tave_annual), 2),
            qltmean = round(mean(qfrac), 3)
            ) %>%
  ungroup()

# For possible future use, compute slopes of q_mm for individual runs ---------
#   - use 1980 - 2017 or 1950 - 2017

trends_start <- 1950 # 1950 seems to give stronger results?

q_runs_wide.df <- q_met.fannual.df00 %>%
  mutate(q_mm = qfrac*qbar) %>% 
  select(year, run, q_mm) %>%
  filter(year >= trends_start & year < 2018) %>%
  spread(key = run, value = q_mm) %>%
  select(-obs) # q simulated from br met data - only to 2000
q_runs_wide.ts <- ts(q_runs_wide.df, start = trends_start, frequency = 1)

# number of runs in the filtered df
runs_n <- length(q_runs_wide.df[1,]) - 1 
# new df to hold slope results
qtrends.df <- data.frame(run = names(q_runs_wide.df[, 2:(runs_n + 1)]),
                         sens_slope = 0.0,
                         sens_ll = -999.9,
                         sens_ul = 999.9,
                         sens_pval = -999.9,
                         lm_slope = 0.0,
                         lm_se = 999.9,
                         lm_pval = -999.9)

for (i in 1:runs_n) {
  # first Sen's nonparametric slope:
  test_q <- sens.slope(q_runs_wide.ts[ , i+1], 0.80)
  qtrends.df$sens_slope[i] <- round(test_q$estimates, 3)
  qtrends.df$sens_ll[i] <- round(test_q$conf.int[1], 3)
  qtrends.df$sens_ul[i] <- round(test_q$conf.int[2], 3)
  qtrends.df$sens_pval[i] <- round(test_q$p.value, 3)
  
  # now a simple least squares slope:
  test_lm <- summary( lm(q_runs_wide.ts[ , i+1] ~ q_runs_wide.ts[ , 1]) )
  qtrends.df$lm_slope[i] <- round(test_lm$coefficients[2,1], 3)
  qtrends.df$lm_se[i] <- round(test_lm$coefficients[2,2], 3)
  qtrends.df$lm_pval[i] <- round(test_lm$coefficients[2,4], 3)
}

# Create df of base = 1965, 1995, and 2050 qfrac_ltmeans for each run ---------
qltmean.base_2050.df <- q.frun.ltstats0.df %>%
  dplyr::filter(era == 1965 | era == 1995 | era == 2050) %>%
  dplyr::select(-pltmean, -tltmean) %>%
  tidyr::spread(key = "era", value = "qltmean")
names(qltmean.base_2050.df) <- c("run", "qfrac_ltmean_base_orig","qfrac_ltmean_1995_orig", "qfrac_ltmean_2050_orig")

# Add the qfrac_ltmean orig's to the filtered annual df -----------------------
q_met.fannual.df0 <- left_join(q_met.fannual.df00, qltmean.base_2050.df, 
                           by = c("run"))

# Right now I am NOT RENORMALIZING: -------------------------------------------
# "Renormalize" annual dq values so that dqltmean is 0 for base era
# q_met.fannual.df <- q_met.fannual.df %>%
#   dplyr::mutate(dqr = dq - dqltmean_base_orig, 
#                 dqltmean_2050 = dqltmean_2050_orig - dqltmean_base_orig) 
# q_met.fannual.df <- q_met.fannual.df %>%
#   dplyr::mutate(dqr = dq, 
#                 dqltmean_2050 = dqltmean_2050_orig) 

# Right now I am NOT RENORMALIZING: -------------------------------------------
#    - if I were renormalizing, the following would make more sense
q_met.fannual.df0 <- q_met.fannual.df0 %>%
  dplyr::mutate(qfracr = qfrac, qfrac_ltmean_1965 = qfrac_ltmean_base_orig,
                qfrac_ltmean_1995 = qfrac_ltmean_1995_orig,
                qfrac_ltmean_2050 = qfrac_ltmean_2050_orig,
                qfrac_ltmean_change_1995 = qfrac_ltmean_1995 - qfrac_ltmean_1965,
                qfrac_ltmean_change_2050 = qfrac_ltmean_2050 - qfrac_ltmean_1965) %>%
  select(-qfrac_ltmean_base_orig, -qfrac_ltmean_1995_orig, -qfrac_ltmean_2050_orig)

# Compute long-term stats for renormalized qfrac's ----------------------------
q.frun.ltstats.df <- q_met.fannual.df0 %>%
  group_by(run, era, rcp) %>%
  summarise(pltmean = mean(pave_annual),
            tltmean = mean(tave_annual),
            qltmean = mean(qfracr),
            qltsd = sd(qfracr),
            qlt975 = quantile(qfracr, probs = 0.975, na.rm = TRUE),
            qlt95 = quantile(qfracr, probs = 0.95, na.rm = TRUE),
            qlt90 = quantile(qfracr, probs = 0.90, na.rm = TRUE),
            qlt75 = quantile(qfracr, probs = 0.75, na.rm = TRUE),
            qlt50 = quantile(qfracr, probs = 0.50, na.rm = TRUE),
            qlt25 = quantile(qfracr, probs = 0.25, na.rm = TRUE),
            qlt10 = quantile(qfracr, probs = 0.10, na.rm = TRUE),
            qlt05 = quantile(qfracr, probs = 0.05, na.rm = TRUE),
            qlt025 = quantile(qfracr, probs = 0.025, na.rm = TRUE),
            qltmin = min(qfracr)
            ) %>%
  mutate_at(4:17, round, 2) %>%
  ungroup()

# Add stats to the filtered annual df -----------------------------------------
q_met.fannual.df <- left_join(q_met.fannual.df0, q.frun.ltstats.df, 
                           by = c("run", "era"))

# # Test drought persistence metric --------------------------------------------
# #    (dpersist = 1 if the current year AND the next year are dry)
# q_met.fannual.drytest <- q_met.fannual.df %>%
#   dplyr::arrange(run, year) %>%
#   mutate(dry = if_else(dq <= dqlt25, 1, 0),
#          nextdry = lead(dry, 1),
#          nextrun = lead(run, 1),
#          dpersist = if_else(dry == 1 & nextdry == 1, 1, 0),
#          dpersist = if_else(nextrun == run, dpersist, 0)) %>%
#   select(-pltmean_base, -tltmean_base, -dp, -dt, 
#          -pave_annual, -tave_annual)
# 
# # For each run, find average dry & dpersist in eras ---------------------------
# #    (result is total dpersist per decade)
# q.frun.drytest <- q_met.fannual.drytest %>%
#   select(run, rcp, era, dqltsd, dry, dpersist) %>%
#   group_by(run, rcp, era) %>%
#   summarise(sum_persist = round(sum(dpersist)/2, 1),
#             sum_dry = round(sum(dry)/2, 1)) %>%
#   ungroup()
# 
# # Add to original q.frun.stats.df -------------------------------------------
# q.frun.ltstats.df <- left_join(q.frun.ltstats.df, q.frun.drytest, 
#                                by = c("run", "era"))
# 
# Summarise lt stats by rcp ---------------------------------------------------
#   - this is just really for graphing
q.rcpmedians <- q.frun.ltstats.df %>%
  group_by(rcp, era) %>%
  summarise(dqltmean_rcpmedian = median(qltmean),
            qltsd_rcpmedian = median(qltsd),
            qltmin_rcpmedian = median(qltmin),
            qlt05_rcpmedian = median(qlt025),
            qlt05_rcpmedian = median(qlt05),
            qlt25_rcpmedian = median(qlt25),
            qlt50_rcpmedian = median(qlt50),
            qlt75_rcpmedian = median(qlt75),
            qlt95_rcpmedian = median(qlt95),
            qlt975_rcpmedian = median(qlt975)
            # dry_rcpmedian = round(median(sum_dry),3),
            # dpersist_rcpmedian = round(median(sum_persist), 3)) %>%
  ) %>%
  ungroup()

# Calculate quantiles of qltmean == qfrac_ltmean ------------------------------
#   - this df provides the limits used to define cc scenarios
#   - delete rcp = obs_prism and obs_br
qfrac_ltmean.stats <- q.frun.ltstats.df %>%
  filter(!(rcp == "obs_prism")) %>%
  filter(!(rcp == "obs_br")) %>%
  group_by(era) %>%
  summarise(qltmean_min = min(qltmean),
            qltmean_025 = quantile(qltmean, probs = 0.025, na.rm = TRUE),
            qltmean_050 = quantile(qltmean, probs = 0.05, na.rm = TRUE),
            qltmean_100 = quantile(qltmean, probs = 0.10, na.rm = TRUE),
            qltmean_175 = quantile(qltmean, probs = 0.175, na.rm = TRUE),
            qltmean_200 = quantile(qltmean, probs = 0.20, na.rm = TRUE),
            qltmean_250 = quantile(qltmean, probs = 0.25, na.rm = TRUE),
            qltmean_325 = quantile(qltmean, probs = 0.325, na.rm = TRUE),
            qltmean_400 = quantile(qltmean, probs = 0.400, na.rm = TRUE),
            qltmean_425 = quantile(qltmean, probs = 0.425, na.rm = TRUE),
            qltmean_500 = quantile(qltmean, probs = 0.50, na.rm = TRUE),
            qltmean_575 = quantile(qltmean, probs = 0.575, na.rm = TRUE),
            qltmean_600 = quantile(qltmean, probs = 0.600, na.rm = TRUE),
            qltmean_675 = quantile(qltmean, probs = 0.675, na.rm = TRUE),
            qltmean_750 = quantile(qltmean, probs = 0.75, na.rm = TRUE),
            qltmean_800 = quantile(qltmean, probs = 0.800, na.rm = TRUE),
            qltmean_825 = quantile(qltmean, probs = 0.825, na.rm = TRUE),
            qltmean_900 = quantile(qltmean, probs = 0.90, na.rm = TRUE),
            qltmean_950 = quantile(qltmean, probs = 0.95, na.rm = TRUE),
            qltmean_975 = quantile(qltmean, probs = 0.975, na.rm = TRUE),
            qltmean_max = max(qltmean)) %>%
  mutate_all(round, 2) %>%
  ungroup()

# Prepare for graphing --------------------------------------------------------
#   (dry and dpersist are so boring don't bother graphing)
q.rcp.stats.long <- q.rcpmedians %>%
  tidyr::gather(key = "stat", value = "val", -rcp, -era)

q_mean_sd <- q.rcp.stats.long %>%
    dplyr::filter(stat == "qltmean_rcpmedian" | stat == "qltsd_rcpmedian")
q_q05_q95 <- q.rcp.stats.long %>%
    dplyr::filter(stat == "qlt05_rcpmedian" | stat == "qlt25_rcpmedian"
                  | stat == "qlt50_rcpmedian" | stat == "qlt75_rcpmedian" 
                  | stat == "qlt95_rcpmedian" | stat == "qltmin_rcpmedian")

qfrac_ltmean.stats.long <- qfrac_ltmean.stats %>%
  tidyr::gather(key = "stat", value = "val", -era)

qfrac_ltmean_q05_q95 <- qfrac_ltmean.stats.long %>%
    dplyr::filter(stat == "qltmean_050" | stat == "qltmean_100"
                  | stat == "qltmean_250"
                  | stat == "qltmean_500" | stat == "qltmean_750"
                  | stat == "qltmean_900" | stat == "qltmean_950")

```

First, a table of the stats of the long-term run means:

```{r}
# Print out the stats of the long-term mean dq's
knitr::kable(qfrac_ltmean.stats)
```

```{r}
# ggplot(data = q_mean_sd, aes(x = era, y = val))  +
#   geom_line(aes(linetype = rcp, colour = stat)) +
#   ggtitle("dq medians by RCP of filtered run mins and stdevs") +
#   labs(x = "Year", y = "Percent change in flow from mean baseline")
ggplot(data = q_q05_q95, aes(x = era, y = val))  +
  geom_line(aes(linetype = rcp, colour = stat)) +
  ggtitle("Medians of filtered runs long-term quantiles") +
  labs(x = "Year", y = "Percent change in flow from mean baseline")
ggplot(data = qfrac_ltmean_q05_q95, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("Quantiles of all filtered run long-term means") +
  labs(x = "Year", y = "Percent change in flow from mean baseline")
```
## Climate change scenarios

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Develop climate change scenarios
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# CAREFUL! - row 4 is 2050 right now but could change -------------------------
#    - Now grouping by ltmean in era = 2050
limits <- qfrac_ltmean.stats[4,] 
cc90_lowerlim <- limits$qltmean_025[1]
cc90_upperlim <- limits$qltmean_175[1]
cc75_lowerlim <- limits$qltmean_175[1]
cc75_upperlim <- limits$qltmean_325[1]
cc50_lowerlim <- limits$qltmean_425[1]
cc50_upperlim <- limits$qltmean_575[1]
cc25_lowerlim <- limits$qltmean_675[1]
cc25_upperlim <- limits$qltmean_825[1]
cc10_lowerlim <- limits$qltmean_825[1]
cc10_upperlim <- limits$qltmean_975[1]


# Look at pools of run results, by year, --------------------------------------
#    with 1995 lt means close to 10, 25, 50, 75, and 90th percentiles

q_cc90.runs.df <- q_met.fannual.df %>%
  dplyr::filter(qfrac_ltmean_2050 >= cc90_lowerlim &
                  qfrac_ltmean_2050 < cc90_upperlim) %>%
  dplyr::select(year, era, run, qfracr, pave_annual, tave_annual)

q_cc75.runs.df <- q_met.fannual.df %>%
  dplyr::filter(qfrac_ltmean_2050 >= cc75_lowerlim &
                  qfrac_ltmean_2050 < cc75_upperlim) %>%
  dplyr::select(year, era, run, qfracr, pave_annual, tave_annual)

q_cc50.runs.df <- q_met.fannual.df %>%
  dplyr::filter(qfrac_ltmean_2050 >= cc50_lowerlim &
                  qfrac_ltmean_2050 < cc50_upperlim) %>%
  dplyr::select(year, era, run, qfracr, pave_annual, tave_annual)

q_cc25.runs.df <- q_met.fannual.df %>%
  dplyr::filter(qfrac_ltmean_2050 >= cc25_lowerlim &
                  qfrac_ltmean_2050 < cc25_upperlim) %>%
  dplyr::select(year, era, run, qfracr, pave_annual, tave_annual)

q_cc10.runs.df <- q_met.fannual.df %>%
  dplyr::filter(qfrac_ltmean_2050 >= cc10_lowerlim &
                  qfrac_ltmean_2050 <= cc10_upperlim) %>%
  dplyr::select(year, era, run, qfracr, pave_annual, tave_annual)

q_cc90 <- q_cc90.runs.df %>%
  group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
            ) %>%
  ungroup()

q_cc90_long <- q_cc90 %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)

q_cc75 <- q_cc75.runs.df %>%
  group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
              ) %>%
  ungroup()
q_cc75_long <- q_cc75 %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)

q_cc50 <- q_cc50.runs.df %>%
  group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
              ) %>%
  ungroup()
q_cc50_long <- q_cc50 %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)

q_cc25 <- q_cc25.runs.df %>%
  group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
              ) %>%
  ungroup()
q_cc25_long <- q_cc25 %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)

q_cc10 <- q_cc10.runs.df %>%
  group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
              ) %>%
  ungroup()
q_cc10_long <- q_cc10 %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)

q_ccall <- q_met.fannual.df %>%
    group_by(era) %>%
  summarise(dqrlt001 = round(quantile(qfracr, probs = 0.001, na.rm = TRUE), 2),
            dqrlt01 = round(quantile(qfracr, probs = 0.01, na.rm = TRUE), 2),
            dqrlt02 = round(quantile(qfracr, probs = 0.02, na.rm = TRUE), 2),
            dqrlt05 = round(quantile(qfracr, probs = 0.05, na.rm = TRUE), 2),
            dqrlt10 = round(quantile(qfracr, probs = 0.10, na.rm = TRUE), 2),
            dqrlt25 = round(quantile(qfracr, probs = 0.25, na.rm = TRUE), 2),
            dqrlt35 = round(quantile(qfracr, probs = 0.35, na.rm = TRUE), 2),
            dqrlt50 = round(quantile(qfracr, probs = 0.50, na.rm = TRUE), 2),
            dqrlt65 = round(quantile(qfracr, probs = 0.65, na.rm = TRUE), 2),
            dqrlt75 = round(quantile(qfracr, probs = 0.75, na.rm = TRUE), 2),
            dqrlt90 = round(quantile(qfracr, probs = 0.90, na.rm = TRUE), 2),
            dqrlt95 = round(quantile(qfracr, probs = 0.95, na.rm = TRUE), 2),
            dqrlt98 = round(quantile(qfracr, probs = 0.98, na.rm = TRUE), 2),
            dqrlt99 = round(quantile(qfracr, probs = 0.99, na.rm = TRUE), 2),
            dqrlt999 = round(quantile(qfracr, probs = 0.999, na.rm = TRUE), 2),
            # dqrmax = round(max(qfracr), 2), 
            dqrcount = n(),
            dqrmean = round(mean(qfracr), 2),
            dqrsd = round(sd(qfracr), 2),
            pltave = round(mean(pave_annual)),
            tltave = round(mean(tave_annual), 2)
              ) %>%
  ungroup()
q_cc_all_long <- q_ccall %>%
  dplyr::mutate(era = as.integer(era)) %>%
  dplyr::select(-dqrcount, -dqrsd, -pltave, -tltave,
                -dqrlt35, -dqrlt65, -dqrmean) %>%
  tidyr::gather(key = "stat", value = "val", -era)
```

## Tables of the quantile values by era
```{r}
knitr::kable(q_cc90)
knitr::kable(q_cc75)
knitr::kable(q_cc50)
knitr::kable(q_cc25)
knitr::kable(q_cc10)

```

### Plot the filtered runs, grouped by the long-term mean flow in era=2050:
```{r}
qfrac_ltmean_q100 <- qfrac_ltmean_q05_q95 %>%
    dplyr::filter(stat == "qltmean_100")
qfrac_ltmean_q250 <- qfrac_ltmean_q05_q95 %>%
    dplyr::filter(stat == "qltmean_250")
qfrac_ltmean_q500 <- qfrac_ltmean_q05_q95 %>%
    dplyr::filter(stat == "qltmean_500")
qfrac_ltmean_q750 <- qfrac_ltmean_q05_q95 %>%
    dplyr::filter(stat == "qltmean_750")
qfrac_ltmean_q900 <- qfrac_ltmean_q05_q95 %>%
    dplyr::filter(stat == "qltmean_900")
ggplot(data = q_cc90.runs.df, aes(x = year, y = qfracr))  +
  geom_line(aes(colour = run)) +
  geom_line(data = qfrac_ltmean_q100, aes(x = era, y = val)) +
  # ggtitle("dqr renormalized 15% of runs centered around 10th percentile mean") +
  labs(x = "Year", y = "Fractional change in flow from mean baseline") +
  scale_y_continuous(limits = c(0, 3))
ggplot(data = q_cc75.runs.df, aes(x = year, y = qfracr))  +
  geom_line(aes(colour = run)) +
  geom_line(data = qfrac_ltmean_q250, aes(x = era, y = val)) +
  ggtitle("dqr renormalized 15% of runs centered around 25th percentile mean") +
  labs(x = "Year", y = "Fractional change in flow from mean baseline") +
  scale_y_continuous(limits = c(0, 3))
ggplot(data = q_cc50.runs.df, aes(x = year, y = qfracr))  +
  geom_line(aes(colour = run)) +
  geom_line(data = qfrac_ltmean_q500, aes(x = era, y = val)) +
  # ggtitle("dqr renormalized 15% of runs centered around 50th percentile mean") +
  labs(x = "Year", y = "Fractional change in flow from mean baseline") +
  scale_y_continuous(limits = c(0, 3))
ggplot(data = q_cc25.runs.df, aes(x = year, y = qfracr))  +
  geom_line(aes(colour = run)) +
  geom_line(data = qfrac_ltmean_q750, aes(x = era, y = val)) +
  ggtitle("dqr renormalized 15% of runs centered around 75th percentile mean") +
  labs(x = "Year", y = "Fractional change in flow from mean baseline") +
  scale_y_continuous(limits = c(0, 3))
ggplot(data = q_cc10.runs.df, aes(x = year, y = qfracr))  +
  geom_line(aes(colour = run)) +
  geom_line(data = qfrac_ltmean_q900, aes(x = era, y = val)) +
  # ggtitle("dqr renormalized 15% of runs centered around 90th percentile mean") +
  labs(x = "Year", y = "Fractional change in flow from mean baseline") +
  scale_y_continuous(limits = c(0, 3))
```

### Plot the run quantiles for each scenario 

```{r}
ggplot(data = q_cc10_long, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("cc10: quantiles of long-term mean dq's") +
  labs(x = "Year", y = "Percent change in flow") +
  scale_y_continuous(limits = c(0.0, 2.8), breaks = seq(0.0, 2.8, 0.2))
ggplot(data = q_cc25_long, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("cc25: quantiles of long-term mean dq's") +
  labs(x = "Year", y = "Percent change in flow") +
  scale_y_continuous(limits = c(0.0, 2.8), breaks = seq(0.0, 2.8, 0.2))
ggplot(data = q_cc50_long, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("cc50: Annual dq quantiles of runs with lt means near 50th percentile") +
  labs(x = "Year", y = "Percent change in flow") +
  scale_y_continuous(limits = c(0.0, 2.8), breaks = seq(0.0, 2.8, 0.2))
ggplot(data = q_cc75_long, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("cc75: quantiles of long-term mean dq's") +
  labs(x = "Year", y = "Percent change in flow") +
  scale_y_continuous(limits = c(0.0, 2.8), breaks = seq(0.0, 2.8, 0.2))
ggplot(data = q_cc90_long, aes(x = era, y = val))  +
  geom_line(aes(colour = stat)) +
  ggtitle("cc90: quantiles of long-term mean dq's") +
  labs(x = "Year", y = "Percent change in flow") +
  scale_y_continuous(limits = c(0.0, 2.8), breaks = seq(0.0, 2.8, 0.2))
```

```{r include=FALSE}
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Calculate metrics for scenario selection - SKIP FOR NOW
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Look at observed q_mm linear model ------------------------------------------

# First need mean lfalls q_mm for 1897-1979 (model calibration period)
#   - this should be the same as qbar in climate sensitivity model
qbar_base <- lfalls_annual.df0 %>%
  filter(year > 1896 & year < 1980)
qbar_base <- mean(qbar_base$qave_annual_mm)

# Compute slope of observed q_mm - linear model - for 2 possible time periods
lfalls_annual_1950to2017.df <-  lfalls_annual.df0 %>%
  filter(year >= 1950 & year < 2018) %>%
  select(-qave_annual_cfs)
lfalls_obs_lm1 <- summary( lm(formula = q_mm ~ year, 
                             data = lfalls_annual_1950to2017.df) )
qobs_slope <- lfalls_obs_lm1$coefficients[2, 1]
qobs_se <- lfalls_obs_lm1$coefficients[2, 2]
qobs_tval <- lfalls_obs_lm1$coefficients[2, 3]
qobs_pval <- lfalls_obs_lm1$coefficients[2, 4]

lfalls_annual_1980to2017.df <-  lfalls_annual.df0 %>%
  filter(year >= 1980 & year < 2018) %>%
  select(-qave_annual_cfs)

lfalls_obs_lm2 <- summary( lm(formula = q_mm ~ year, 
                             data = lfalls_annual_1980to2017.df) )

# Also look at the Theil-Sen nonparametric slope ------------------------------
#    - need table to be in "time series" format for Sens slope
lfalls.ts <- ts(lfalls_annual_1950to2017.df, start =1950, frequency = 1)
lfalls_trends_sens <- sens.slope(lfalls.ts[, 2], 0.80)

# For now, here are the 1950-2017 lm results
qobs_slope <- 0.6037
qobs_se <- 0.7851

# Look at q_mm linear models for each scenario ensemble of runs ---------------
q_cc90_trends.df <- q_cc90.runs.df %>%
  filter(year < 2018) %>%
  mutate(q_mm = qbar*qfracr) %>%
  select(year, run, q_mm)
cc90_lm <- summary( lm(formula = q_mm ~ year, data = q_cc90_trends.df) )
cc90_results <- cc90_lm$coefficients # slope = -0.23, pval = 0.022

q_cc75_trends.df <- q_cc75.runs.df %>%
  filter(year < 2018) %>%
  mutate(q_mm = qbar*qfracr) %>%
  select(year, run, q_mm)
cc75_lm <- summary( lm(formula = q_mm ~ year, data = q_cc75_trends.df) )
cc75_results <- cc75_lm$coefficients # slope = 0.18, pval = 0.09

q_cc50_trends.df <- q_cc50.runs.df %>%
  filter(year < 2018) %>%
  mutate(q_mm = qbar*qfracr) %>%
  select(year, run, q_mm)
cc50_lm <- summary( lm(formula = q_mm ~ year, data = q_cc50_trends.df) )
cc50_results <- cc50_lm$coefficients # slope = -0.10, pval = 0.36

q_cc25_trends.df <- q_cc25.runs.df %>%
  filter(year < 2018) %>%
  mutate(q_mm = qbar*qfracr) %>%
  select(year, run, q_mm)
cc25_lm <- summary( lm(formula = q_mm ~ year, data = q_cc25_trends.df) )
cc25_results <- cc25_lm$coefficients # slope = 0.34, pval = 0.001

q_cc10_trends.df <- q_cc10.runs.df %>%
  filter(year < 2018) %>%
  mutate(q_mm = qbar*qfracr) %>%
  select(year, run, q_mm)
cc10_lm <- summary( lm(formula = q_mm ~ year, data = q_cc10_trends.df) )
cc10_results <- cc10_lm$coefficients # slope = 0.47, pval = 0.000

# Also try the ANCOVA method for comparing 2 lines ----------------------------
# (http://r-eco-evo.blogspot.com/2011/08/comparing-two-regression-slopes-by.html)

# First need to combine obs and scenario
#   - need the type column to be a "factor"
q_obs_ancova.df <- lfalls_annual_1950to2017.df %>%
  mutate(type = "obs")

q_cc10_ancova.df0 <- q_cc10_trends.df %>%
  mutate(type = "sim") %>%
  select(-run)
q_cc10_ancova.df <- bind_rows(q_cc10_ancova.df0, q_obs_ancova.df)
q_cc10_ancova.df$type <- as.factor(q_cc10_ancova.df$type)
cc10_mod1 <- aov(formula = q_mm ~ year*type, 
                     data = q_cc10_ancova.df)
cc10_mod2 <- aov(formula = q_mm ~ year + type, 
                     data = q_cc10_ancova.df)
summary(cc10_mod1)
summary(cc10_mod2)
anova(cc10_mod1, cc10_mod2)

q_cc25_ancova.df0 <- q_cc25_trends.df %>%
  mutate(type = "sim") %>%
  select(-run)
q_cc25_ancova.df <- bind_rows(q_cc25_ancova.df0, q_obs_ancova.df)
q_cc25_ancova.df$type <- as.factor(q_cc25_ancova.df$type)
cc25_mod1 <- aov(formula = q_mm ~ year*type, 
                     data = q_cc25_ancova.df)
cc25_mod2 <- aov(formula = q_mm ~ year + type, 
                     data = q_cc25_ancova.df)
summary(cc25_mod1)
summary(cc25_mod2)
anova(cc25_mod1, cc25_mod2)

q_cc50_ancova.df0 <- q_cc50_trends.df %>%
  mutate(type = "sim") %>%
  select(-run)
q_cc50_ancova.df <- bind_rows(q_cc50_ancova.df0, q_obs_ancova.df)
q_cc50_ancova.df$type <- as.factor(q_cc50_ancova.df$type)
cc50_mod1 <- aov(formula = q_mm ~ year*type, 
                     data = q_cc50_ancova.df)
cc50_mod2 <- aov(formula = q_mm ~ year + type, 
                     data = q_cc50_ancova.df)
summary(cc50_mod1)
summary(cc50_mod2)
anova(cc50_mod1, cc50_mod2)

q_cc75_ancova.df0 <- q_cc75_trends.df %>%
  mutate(type = "sim") %>%
  select(-run)
q_cc75_ancova.df <- bind_rows(q_cc75_ancova.df0, q_obs_ancova.df)
q_cc75_ancova.df$type <- as.factor(q_cc75_ancova.df$type)
cc75_mod1 <- aov(formula = q_mm ~ year*type, 
                     data = q_cc75_ancova.df)
cc75_mod2 <- aov(formula = q_mm ~ year + type, 
                     data = q_cc75_ancova.df)
summary(cc75_mod1)
summary(cc75_mod2)
anova(cc75_mod1, cc75_mod2)

q_cc90_ancova.df0 <- q_cc90_trends.df %>%
  mutate(type = "sim") %>%
  select(-run)
q_cc90_ancova.df <- bind_rows(q_cc90_ancova.df0, q_obs_ancova.df)
q_cc90_ancova.df$type <- as.factor(q_cc90_ancova.df$type)
cc90_mod1 <- aov(formula = q_mm ~ year*type, 
                     data = q_cc90_ancova.df)
cc90_mod2 <- aov(formula = q_mm ~ year + type, 
                     data = q_cc90_ancova.df)
summary(cc90_mod1)
summary(cc90_mod2)
anova(cc90_mod1, cc90_mod2)

#Let me try this on my own using lm -------------------------------------------
q_obs_lm.df <- lfalls_annual_1950to2017.df %>%
  mutate(type = 1)
q_cc10_lm.df0 <- q_cc10_trends.df %>%
  mutate(type = 0) %>%
  select(-run)
q_cc10_lm.df <- bind_rows(q_cc10_lm.df0, q_obs_lm.df)
cc10_lm <- summary ( lm(formula = q_mm ~ type + year*type, 
                     data = q_cc10_lm.df))
cc10_lm$coefficients

q_cc25_lm.df0 <- q_cc25_trends.df %>%
  mutate(type = 0) %>%
  select(-run)
q_cc25_lm.df <- bind_rows(q_cc25_lm.df0, q_obs_lm.df)
cc25_lm <- summary ( lm(formula = q_mm ~ type + year*type, 
                     data = q_cc25_lm.df))
cc25_lm$coefficients

q_cc50_lm.df0 <- q_cc50_trends.df %>%
  mutate(type = 0) %>%
  select(-run)
q_cc50_lm.df <- bind_rows(q_cc50_lm.df0, q_obs_lm.df)
cc50_lm <- summary ( lm(formula = q_mm ~ type + year*type, 
                     data = q_cc50_lm.df))
cc50_lm$coefficients

q_cc75_lm.df0 <- q_cc75_trends.df %>%
  mutate(type = 0) %>%
  select(-run)
q_cc75_lm.df <- bind_rows(q_cc75_lm.df0, q_obs_lm.df)
cc75_lm <- summary ( lm(formula = q_mm ~ type + year*type, 
                     data = q_cc75_lm.df))
cc75_lm$coefficients

q_cc90_lm.df0 <- q_cc90_trends.df %>%
  mutate(type = 0) %>%
  select(-run)
q_cc90_lm.df <- bind_rows(q_cc90_lm.df0, q_obs_lm.df)
cc90_lm <- summary ( lm(formula = q_mm ~ type + year*type, 
                     data = q_cc90_lm.df))
cc90_lm$coefficients

# Compute q scale factors, from 1965 to 2050, for cc50 ------------------------
# I moved this to make_prrism_inputs

# quantiles <- c(0.001, 0.01, 0.02, 0.05, 0.10, 0.25, 0.35, 0.50,
#                0.65, 0.75, 0.90, 0.95, 0.98, 0.99, 1.00) # dqlt999 -> 1.00
# 
# qm_cc50 <- q_cc50 %>% 
#   filter(era %in% c(1965, 2050)) %>%
#   mutate(eratxt = c("t1965", "t2050")) %>%  
#   select(-dqrcount, -dqrmean, -dqrsd, -pltave, -tltave, -era) %>%
#   gather(newrows, values, -eratxt) %>%
#   spread(eratxt, values)
# 
# # Add column with numeric quantile values to replace text quantile values
# #   - e.g. dqrlt01 -> 0.01; compute the quantile mapping values
# qm_cc50 <- qm_cc50 %>%
#   cbind(quantiles) %>%
#   select(newrows, quantiles, t1965, t2050) %>%
#   mutate(qscale_cc50 = round(t2050/t1965, 2))

```

### Flow change factors for "no change" scenario (cc50)

```{r}
knitr::kable(qm_cc50)
```

```{r}
# Remove objects that are no longer useful to clean up the global environment.
#rm(with.df, withdrawals.df, pot.total)
```

